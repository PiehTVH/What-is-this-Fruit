{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCcNBGdsYYw7lb7YeewISj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiehTVH/What-is-this-Fruit/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remember to read the kaggle API token to interact with your kaggle account."
      ],
      "metadata": {
        "id": "qL0kiQU3h_rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d 'moltean/fruits'"
      ],
      "metadata": {
        "id": "zR58TE1IF40Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/fruits.zip"
      ],
      "metadata": {
        "id": "M88StZHz1Kln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "2-g1sKcV1Nim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geffnet"
      ],
      "metadata": {
        "id": "M4ZbSKcN1fiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "YiKakZiM1uSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import PIL\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "import pickle\n",
        "import geffnet\n",
        "import logging\n",
        "import fnmatch\n",
        "import argparse\n",
        "import torchvision\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from copy import deepcopy\n",
        "from sklearn import metrics\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from geffnet import create_model\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from torch.optim import lr_scheduler\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2_hHHbfGHG97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/fruits-360_dataset/fruits-360'\n",
        "train_dir = data_dir + '/Training'\n",
        "valid_dir = data_dir + '/Test'\n",
        "\n",
        "data_transforms = {\n",
        "    'Training': transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'Test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['Training', 'Test']}\n",
        "\n",
        "batch_size = 64\n",
        "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
        "              for x in ['Training', 'Test']} \n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['Training', 'Test']}\n",
        "\n",
        "class_names = image_datasets['Test'].classes"
      ],
      "metadata": {
        "id": "alHM6WzA1J1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save file labels\n",
        "with open('labels.txt', 'w') as f:\n",
        "  for i in class_names:\n",
        "    f.write(i + '\\n')"
      ],
      "metadata": {
        "id": "iNDW9Y1_fnY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = image_datasets['Training'].class_to_idx\n",
        "cat_to_name = {_[i]: i for i in list(_.keys())}\n",
        "class_to_idx = {str(i): i for i in range(len(class_names))}\n",
        "\n",
        "# Run this to test the data loader\n",
        "images, labels = next(iter(data_loader['Test']))\n",
        "images.size()"
      ],
      "metadata": {
        "id": "pnLMXoJ75g37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save file json: cat_to_name\n",
        "with open(\"cat_to_name.json\", \"w\") as outfile:\n",
        "    json.dump(cat_to_name, outfile)\n",
        "\n",
        "with open(\"class_to_idx.json\", \"w\") as outfile:\n",
        "    json.dump(class_to_idx, outfile)"
      ],
      "metadata": {
        "id": "nnOggzkebYa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dr2soRwb6Wpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showimage(data_loader, number_images, cat_to_name):\n",
        "    images, labels = next(iter(data_loader))\n",
        "    images = images.numpy() # convert images to numpy for display\n",
        "    # plot the images in the batch, along with the corresponding labels\n",
        "    fig = plt.figure(figsize=(number_images, 4))\n",
        "    # display 20 images\n",
        "    for idx in np.arange(number_images):\n",
        "        ax = fig.add_subplot(2, int(number_images/2), idx+1, xticks=[], yticks=[])\n",
        "        img = np.transpose(images[idx])\n",
        "        plt.imshow(img)\n",
        "        ax.set_title(cat_to_name[labels.tolist()[idx]])\n",
        "        \n",
        "\n",
        "#### to show some  images\n",
        "showimage(data_loader['Test'],2,cat_to_name)"
      ],
      "metadata": {
        "id": "4oXZWJ0P57p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model('efficientnet_b0', pretrained=True)\n",
        "# Create classifier\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "n_classes = 131\n",
        "model.classifier = nn.Linear(model.classifier.in_features, n_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), \n",
        "                      lr=0.001,momentum=0.9,\n",
        "                      nesterov=True,\n",
        "                      weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "metadata": {
        "id": "F2YfGHkk578m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.class_to_idx = image_datasets['Training'].class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "list(model.class_to_idx.items())"
      ],
      "metadata": {
        "id": "Kjh6i-Jv62KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=200, checkpoint=None):\n",
        "  since = time.time()\n",
        "\n",
        "  if checkpoint is None:\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = math.inf\n",
        "    best_acc = 0\n",
        "  else:\n",
        "    print(f'Val loss: {checkpoint[\"best_val_loss\"]}, Val accuracy: {checkpoint[\"best_val_accuracy\"]}')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    best_loss = checkpoint['best_val_loss']\n",
        "    best_acc = checkpoint['best_val_accuracy']\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['Training', 'Test']:\n",
        "      if phase == 'Training':\n",
        "        model.train()       # Set model to training mode\n",
        "      else:\n",
        "        model.eval()        # Set model to evaluate mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_correct = 0\n",
        "\n",
        "      #Iterate over data\n",
        "      for i, (inputs, labels) in enumerate(data_loader[phase]):\n",
        "        inputs = inputs.to(device) \n",
        "        labels = labels.to(device) \n",
        "\n",
        "        #zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if i % 1000 == 999:\n",
        "          print('[%d, %d] loss: %.8f' % \n",
        "                          (epoch + 1, i, running_loss / (i * inputs.size(0))))\n",
        "          \n",
        "        #forward\n",
        "        #track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'Training'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          #backward + optimize only if in training phase\n",
        "          if phase == 'Training':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        #statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_correct += torch.sum(preds == labels.data)\n",
        "\n",
        "      if phase == 'Training':\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_correct.double() / dataset_sizes[phase]\n",
        "\n",
        "      print('{} Loss: {:.8f} Acc: {:.8f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "      \n",
        "      #deep copy the model\n",
        "      if phase == 'Test' and epoch_loss < best_loss:\n",
        "        print(f'New best model found!')\n",
        "        print(f'New record loss: {epoch_loss}, previous record loss: {best_loss}')\n",
        "        best_loss = epoch_loss \n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'best_val_loss': best_loss,\n",
        "                    'best_val_accuracy': best_acc,\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),},\n",
        "                   CHECK_POINT_PATH)\n",
        "        print(f'New record loss is SAVED: {epoch_loss}')  \n",
        "\n",
        "\n",
        "      \"\"\"if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                iteration_change_loss = 0\n",
        "\n",
        "            if iteration_change_loss == 10: #choose a number of epochs for patience\n",
        "                print('Early stopping after {0} iterations without the decrease of the val loss'. format(iteration_change_loss))\n",
        "                break\"\"\"\n",
        "    print()\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "      time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:.8f} Best val loss: {:.8f}'.format(best_acc, best_loss))\n",
        "\n",
        "  #Load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, best_loss, best_acc"
      ],
      "metadata": {
        "id": "Kipb2Ryh64eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = '/content/EfficientNet_B0_SGD.pth'\n",
        "try:\n",
        "  checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "  print('checkpoint loaded')\n",
        "except:\n",
        "  checkpoint = None\n",
        "  print('checkpoint not found')\n",
        "if checkpoint == None:\n",
        "  CHECK_POINT_PATH = CHECKPOINT_PATH\n",
        "\n",
        "model, best_val_loss, best_val_acc = train_model(model,\n",
        "                                                 criterion,\n",
        "                                                 optimizer,\n",
        "                                                 scheduler,\n",
        "                                                 num_epochs = 100,\n",
        "                                                 checkpoint = None #torch.load(CHECK_POINT_PATH)\n",
        "                                                 ) \n",
        "torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'best_val_accuracy': best_val_acc,\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            }, CHECK_POINT_PATH)"
      ],
      "metadata": {
        "id": "i5AFeMgz9nN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End"
      ],
      "metadata": {
        "id": "LU-OyRumgjZw"
      }
    }
  ]
}